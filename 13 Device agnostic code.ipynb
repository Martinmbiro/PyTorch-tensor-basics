{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyPw+2HDC/4O1OfM0BXBn885"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# **Device-agnostic Code**\n",
        "> Deep learning algorithms require a lot of numerical computation (matrix multiplication) which could run faster on NVIDIA GPU's with `CUDA`, much faster than they could on a CPU\n",
        "\n",
        "> ðŸ’¡ **Info**  \n",
        "+ `CUDA` is a platform and API that allows NVIDIA GPU's to be used for general purpose computing tasks and not just graphics\n",
        "+ PyTorch also makes it possible to run tensors on Apple Silicon (M1/M2/M3) chips\n",
        "+ Google colab provides access to an NVIDIA GPU, either free or on premium terms"
      ],
      "metadata": {
        "id": "sLqMFyOuy0Ax"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "> ðŸ”¥ **Pro tip**  \n",
        "> Here's how to set up the free GPU on Google Colab\n",
        "+ Go to the 'Runtime' tab\n",
        "+ Select 'Change runtime type'\n",
        "+ Select the hardware of your choice\n",
        "\n",
        "> _For this particular notebook, I've chosen my hardware as `T4 GPU`_"
      ],
      "metadata": {
        "id": "2VfgtQdNzFdg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import torch\n",
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "OuKiMMFMyuTF",
        "outputId": "90a7e926-ed73-4bd6-d09a-2d407f370700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'2.5.1+cu121'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Checking for NVIDIA GPU"
      ],
      "metadata": {
        "id": "AUTOVvLaygjc"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0SPMAVi1yraB",
        "outputId": "89433f95-2e96-4acc-afe4-7897270d461e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sun Dec 15 10:38:19 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   76C    P0              34W /  70W |    121MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ],
      "source": [
        "# to check if we've got access to an NVIDIA GPU:\n",
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Test access to GPU\n",
        "> If the line below return `True`, PyTorch has access to a GPU"
      ],
      "metadata": {
        "id": "Llxao4wx0MjH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# NVIDIA GPU available?\n",
        "torch.cuda.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Wm1v6d2Z0QVi",
        "outputId": "b1044345-e809-4b39-a010-826eacec513b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# how many NVIDIA GPU's avalable?\n",
        "torch.cuda.device_count()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-wNRaEeR0y4G",
        "outputId": "3ab7f484-7dec-4412-e951-87555ba944be"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Apple Silicon GPU available?\n",
        "torch.backends.mps.is_available()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qYlbsRA11kpI",
        "outputId": "13193c78-50fa-401d-d1b5-6c1d7ab1898a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Setting device type\n",
        "> Setting device type depending on the available hardware"
      ],
      "metadata": {
        "id": "pZzQqTan14hM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "if torch.cuda.is_available():\n",
        "  device = 'cuda' # use NVIDIA GPU (if available)\n",
        "elif torch.backends.mps.is_available():\n",
        "  device = 'mps' # use Apple Silicon GPU (if available)\n",
        "else:\n",
        "  device = 'cpu' # otherwise, result to use CPU"
      ],
      "metadata": {
        "id": "c9KdMA2u1_ih"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Putting tensors and models on GPU\n",
        "> [`torch.Tensor.to()`](https://pytorch.org/docs/stable/generated/torch.Tensor.to.html#torch-tensor-to) performs Tensor `torch.dtype` and/or `torch.device` conversion, inferred from the arguments\n",
        "\n",
        "> ðŸ“ **Note**  \n",
        "+ By default, PyTorch models and tensors are created on the CPU\n",
        "+ `torch.Tensor.to()` returns a copy of that tensor, meaning it will be present on both the CPU & GPU (unless it's overwritten)"
      ],
      "metadata": {
        "id": "GlrnnIH-3iAe"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Changing `dtype` with `Tensor.to()`"
      ],
      "metadata": {
        "id": "tVU2x4FpRdfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# create a tensor (int type)\n",
        "a = torch.arange(12).reshape(3, 4)\n",
        "print(a)\n",
        "print(a.dtype, a.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qDudSJJGRAIu",
        "outputId": "3294551e-d74a-4a0f-d569-997837775270"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0,  1,  2,  3],\n",
            "        [ 4,  5,  6,  7],\n",
            "        [ 8,  9, 10, 11]])\n",
            "torch.int64 cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# change dtype\n",
        "a = a.to(dtype=torch.float32)\n",
        "print(a)\n",
        "print(a.dtype, a.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PZqPp9voRs17",
        "outputId": "4cc3c97c-a0cf-45ec-db7a-4164ca6f5edd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]])\n",
            "torch.float32 cpu\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Changing `device` with `Tensor.to()`\n",
        "\n"
      ],
      "metadata": {
        "id": "2FOtJDMkSMDU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "a = a.to(device=device)\n",
        "print(a)\n",
        "print(a.dtype, a.device)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rB29-8JiSRl8",
        "outputId": "b1c3439a-9dbe-4d4f-d14c-cc5319adff8a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tensor([[ 0.,  1.,  2.,  3.],\n",
            "        [ 4.,  5.,  6.,  7.],\n",
            "        [ 8.,  9., 10., 11.]], device='cuda:0')\n",
            "torch.float32 cuda:0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Moving tensors back to CPU\n",
        "> This might be the case when you want to interact with the tensors in [`NumPy`](https://numpy.org), which does not leverage GPU\n",
        "+ Try using [`torch.Tensor.numpy()`](https://pytorch.org/docs/stable/generated/torch.Tensor.numpy.html) on the tensor living on the GPU"
      ],
      "metadata": {
        "id": "Dw52hlUTUCQu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# this returns an error, but luckily, pytorch\n",
        "# gives us a solution to work around this\n",
        "a.numpy()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        },
        "id": "WHeJTOJ5UIT8",
        "outputId": "a6a1b07f-746b-457f-b567-a7971e969a4c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-24-bc9fdec8e6e7>\u001b[0m in \u001b[0;36m<cell line: 3>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# this returns an error, but luckily, pytorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# gives us a solution to work around this\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0ma\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m: can't convert cuda:0 device type tensor to numpy. Use Tensor.cpu() to copy the tensor to host memory first."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# overwriting the variable a to be an ndarray\n",
        "a = a.cpu().numpy()\n",
        "print(a)\n",
        "print(a.dtype)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7WXDWUVEmSD5",
        "outputId": "8bea7aea-6cd9-441e-98d6-f52569f10438"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  1.  2.  3.]\n",
            " [ 4.  5.  6.  7.]\n",
            " [ 8.  9. 10. 11.]]\n",
            "float32\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# checking the type\n",
        "type(a)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O36MG7AjmzCw",
        "outputId": "29bf1037-44d4-4ecb-b2dd-f69920a1a53c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "numpy.ndarray"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "> â–¶ï¸ **Up Next**  \n",
        "\n",
        "> Pytorch workflow with regression"
      ],
      "metadata": {
        "id": "tYNJ0wj8l2yT"
      }
    }
  ]
}